{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "\n",
    "**Question 1.** Write a python program to find out the words after '@' from the below sentences with the use of regex.\n",
    "\n",
    "\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gmail.com', 'yahoo.com', 'hotmail.com', 'ineuron.ai', 'outlook.com']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp=[\"xyz@gmail.com\",\n",
    "\"abc@yahoo.com\",\n",
    "\"xyz@hotmail.com\",\n",
    "\"abc@ineuron.ai\",\n",
    "\"xyz@outlook.com\"]\n",
    "\n",
    "import re\n",
    "new=[]\n",
    "def mail_id(lst):\n",
    "    for i in range(len(lst)):\n",
    "        out=re.split('@',lst[i])[1]\n",
    "        new.append(out)\n",
    "    return new\n",
    "\n",
    "print(mail_id(inp))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2.** Write a python program with the use of regex to take out the word \"New\" from the following sentence.\n",
    "\n",
    "[\"New Delhi is the capital of India\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Delhi is the capital of India\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp=[\"New Delhi is the capital of India\"]\n",
    "\n",
    "import re\n",
    "def new(word,lst):\n",
    "    st=''.join(lst)\n",
    "    out=re.sub(word,'',st)\n",
    "    return out\n",
    "\n",
    "print(new(\"New\",inp))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3.** Create one python program in which you have to lowercase the sentence first and than delete digits from the following sentence.\n",
    "\n",
    "\"In India, 184 people got affected with Corona virus and 4 are died.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in india,  people got affected with corona virus and  are died.\n",
      "in india,  people got affected with corona virus and  are died.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inp=\"In India, 184 people got affected with Corona virus and 4 are died.\"\n",
    "\n",
    "#With regex :\n",
    "def sent(string):\n",
    "    new=string.lower()\n",
    "    return re.sub(r'\\d','',new)\n",
    "\n",
    "print(sent(inp))\n",
    "\n",
    "#*****************************************\n",
    "#Without regex :\n",
    "def sentence(string):\n",
    "    new=string.lower()\n",
    "    return ''.join([i for i in new if not i.isdigit()])\n",
    "\n",
    "print(sentence(inp))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4.** Do stemming, lemmatization and tokenization from the following sentence.\n",
    "\n",
    "\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming : \n",
      "Type - Lancaster : \n",
      "i hop that , when i hav built up my sav , i wil be abl to travel to hawa .\n",
      "\n",
      "\n",
      "Lemmatization : \n",
      "i hope that , when i have built up my saving , i will be able to travel to hawai .\n",
      "\n",
      "\n",
      "Tokenization : \n",
      "Type - Sentence : \n",
      "['i hope that, when i have built up my savings, i will be able to travel to hawai.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "string=\"I hope that, when I have built up my savings, I will be able to travel to Hawai.\"\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer,LancasterStemmer,SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def stemming(inp,type):\n",
    "    print(\"Stemming : \")\n",
    "    porter=PorterStemmer()\n",
    "    lancaster=LancasterStemmer()\n",
    "    snowball=SnowballStemmer(\"english\")\n",
    "    token=nltk.word_tokenize(inp.lower())\n",
    "    if type==\"porter\":\n",
    "        print(\"Type - Porter : \")\n",
    "        stem=[porter.stem(i) for i in token]\n",
    "        return ' '.join(stem)\n",
    "    elif type==\"lancaster\":\n",
    "        print(\"Type - Lancaster : \")\n",
    "        stem=[lancaster.stem(i) for i in token]\n",
    "        return ' '.join(stem)\n",
    "    elif type==\"snowball\":\n",
    "        print(\"Type - Snowball : \")\n",
    "        stem=[snowball.stem(i) for i in token]\n",
    "        return ' '.join(stem)\n",
    "    else:\n",
    "        return 'Invalid Input Parameters'       \n",
    "    \n",
    "        \n",
    "print(stemming(string,\"lancaster\"))\n",
    "print('\\n')\n",
    "\n",
    "#********************************************************\n",
    "def lemmatization(inp):\n",
    "    print(\"Lemmatization : \")\n",
    "    lemma=WordNetLemmatizer()\n",
    "    token=nltk.word_tokenize(inp.lower())\n",
    "    out=[lemma.lemmatize(i) for i in token]\n",
    "    return ' '.join(out)\n",
    "\n",
    "print(lemmatization(string))\n",
    "print('\\n')\n",
    "\n",
    "#********************************************************\n",
    "\n",
    "def tokenization(inp,type):\n",
    "    print(\"Tokenization : \")\n",
    "    if type==\"word\":\n",
    "        print(\"Type - Word : \")\n",
    "        token=nltk.word_tokenize(inp.lower())\n",
    "        return token\n",
    "    elif type==\"sentence\":\n",
    "        print(\"Type - Sentence : \")\n",
    "        return nltk.sent_tokenize(inp.lower())\n",
    "    \n",
    "print(tokenization(string,\"sentence\"))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5.** Create one python program from the following sentence.\n",
    "\n",
    "\"I love NLP, not you\"\n",
    "\n",
    "output : ['I', 'l', 'N', 'n', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'l', 'N', 'n', 'y']\n"
     ]
    }
   ],
   "source": [
    "inp=\"I love NLP, not you\"\n",
    "\n",
    "def first(string):\n",
    "    return [i[0] for i in inp.split()]\n",
    "\n",
    "print(first(inp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
